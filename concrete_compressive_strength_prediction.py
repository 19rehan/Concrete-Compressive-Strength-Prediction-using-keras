# -*- coding: utf-8 -*-
"""Concrete Compressive Strength Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18US7L8RAC6WzKVDU7YZaGcbMXI1K_dsc
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Data load karein (Concrete strength dataset)
concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')

# Pehli 5 rows check karein
print(concrete_data.head())

# Check karein kitni rows aur columns hain
print(concrete_data.shape)

# 1. Check for null values
print(concrete_data.isnull().sum())

# 2. Divide into Predictors (X) and Target (y)
concrete_data_column = concrete_data.columns

# Target ko chor kar baqi sab predictors hain
predictor = concrete_data[concrete_data_column[concrete_data_column != 'Strength']]
target = concrete_data['Strength']

# Predictors ke columns ki tadad save karein (Model ko batane ke liye)
n_cols = predictor.shape[1]
print(f"Number of input features: {n_cols}")

import seaborn as sns
import matplotlib.pyplot as plt

# 1. Correlation Check (Heatmap)
plt.figure(figsize=(10,6))
sns.heatmap(concrete_data.corr(), annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show()

# 2. Skewness Check
print("Data Skewness:\n", predictor.skew())

concrete_data

# 3. Outliers Check (Boxplot)

outlier_check =['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',
       'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength']

plt.figure(figsize=(15,12))

for i, col in enumerate(outlier_check):
    plt.subplot(3, 3, i+1)
    sns.boxplot(data=concrete_data[col])
    plt.title("Outliers in Normalized Features")
plt.tight_layout()
plt.show()

concrete_data['Superplasticizer'].max()

outlier_check =['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',
       'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength']

for i in outlier_check:
  print(concrete_data[i].max())

import matplotlib.pyplot as plt
import seaborn as sns

# List of columns to check
features = ['Cement', 'Blast Furnace Slag', 'Fly Ash', 'Water', 'Superplasticizer',
            'Coarse Aggregate', 'Fine Aggregate', 'Age', 'Strength']

# 3x3 Grid banayein
plt.figure(figsize=(18, 15))

for i, col in enumerate(features):
    plt.subplot(3, 3, i+1)

    # Histogram with Kernel Density Estimate (KDE)
    sns.histplot(concrete_data[col], kde=True, color='teal')

    # Skewness calculate karke title mein likhein
    skew_val = concrete_data[col].skew()
    plt.title(f"{col} (Skew: {skew_val:.2f})")

plt.tight_layout()
plt.show()

# Age ki skewness khatam karne ke liye Log apply karein
concrete_data['Age'] = np.log1p(concrete_data['Age']) # log1p handle karta hai agar value 0 ho

print(f"New Skewness of Age: {concrete_data['Age'].skew():.2f}")

# Correlation Matrix
plt.figure(figsize=(10,8))
sns.heatmap(concrete_data.corr(), annot=True, cmap='Blues')
plt.title("Feature Correlation with Strength")
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(predictor)

scaled_data = pd.DataFrame(scaled_data, columns=predictor.columns)

n_cols = scaled_data.shape[1]
print(f"Number of input features: {n_cols}")

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout
from keras.callbacks import EarlyStopping

model = Sequential()

model.add(Dense(64, activation = 'relu', input_shape = (n_cols,)))
model.add(Dropout(0.2))
model.add(Dense(32, activation = 'relu'))
model.add(Dense(16, activation = 'relu'))
model.add(Dense(8, activation = 'relu'))
model.add(Dense(1))

model.compile(optimizer = 'adam', loss = 'mean_squared_error')

early_stop = EarlyStopping(monitor = 'val_loss', patience =5)

print("Model Architecture is Ready")

history = model.fit(scaled_data, target, validation_split=0.3, epochs=100, callbacks=[early_stop])

import matplotlib.pyplot as plt

# Plotting the Training and Validation Loss
plt.figure(figsize=(8, 5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Training Progress (Loss)')
plt.xlabel('Epochs')
plt.ylabel('Mean Squared Error (MSE)')
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import r2_score, mean_absolute_error

y_pred = model.predict(scaled_data)

mae = mean_absolute_error(target, y_pred)
r2 = r2_score(target, y_pred)

print(f"Mean Absolute Error (MAE): {mae}")
print(f"R-squared (R2) Score: {r2}")

